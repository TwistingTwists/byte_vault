"use strict";(self.webpackChunkbytevault=self.webpackChunkbytevault||[]).push([[4447],{1160:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"streaming-http-to-disk","metadata":{"permalink":"/byte_vault/blog/streaming-http-to-disk","source":"@site/blog/2025-01-10-http-streaming-to-disk.mdx","title":"Streaming HTTP to Disk","description":"theme","date":"2025-01-10T00:00:00.000Z","tags":[{"inline":false,"label":"Rust","permalink":"/byte_vault/blog/tags/rust","description":"Rust lang"}],"readingTime":2.895,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/blog/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"streaming-http-to-disk","title":"Streaming HTTP to Disk","date":"2025-01-10T00:00:00.000Z","authors":["abeeshake"],"tags":["rust"]},"unlisted":false,"nextItem":{"title":"Understanding DDIA - Chapter 01,02","permalink":"/byte_vault/blog/ch-01-ddia"}},"content":"theme \\n\\n```mermaid\\nflowchart TD\\n    A[\\"Start: Initiate Download\\"] --\x3e B[\\"Send GET request using reqwest client\\"]\\n    B --\x3e C[\\"Receive Response object\\"]\\n    C --\x3e D[\\"Convert response body to byte stream using bytes_stream()\\"]\\n    D --\x3e E[\\"Iterate over each chunk asynchronously\\"]\\n    E --\x3e F[\\"Write each chunk to file using tokio::fs::File\\"]\\n    F --\x3e G{\\"More chunks?\\"}\\n    G -- Yes --\x3e E\\n    G -- No --\x3e H[\\"End: Data Saved to Disk\\"]\\n    \\n    style A fill:#f9f,stroke:#333,stroke-width:2px\\n    style H fill:#bbf,stroke:#333,stroke-width:2px\\n\\n```\\n\\n{/* truncate */}\\n\\n\\n## Understanding the Streaming Process\\n\\nIn the provided streaming implementation, the key to avoiding loading the entire response into memory lies in **how the response body is processed**. Let\'s break down the relevant parts of the code:\\n\\n```rust\\n// Send the GET request and get the response\\nlet resp = client\\n    .get(&url)\\n    .header(\\"User-Agent\\", \\"rust-zip-extractor\\")\\n    .send()\\n    .await?\\n    .error_for_status()?; // Ensure the request was successful\\n\\n// Convert the response body into a stream of bytes\\nlet mut stream = resp.bytes_stream();\\n\\n// Iterate over the stream and write each chunk to the file\\nwhile let Some(chunk) = stream.next().await {\\n    let data = chunk?; // Handle potential stream errors\\n    file.write_all(&data).await?;\\n}\\n```\\n\\n### Key Points:\\n\\n1. **`send().await?` Does Not Buffer the Entire Response:**\\n   - The `.send().await?` method initiates the HTTP request and returns a `Response` object **without** reading the entire response body into memory.\\n   - The response body is **lazy-loaded**, meaning it fetches data incrementally as you process the stream.\\n\\n2. **Using `bytes_stream()` for Streaming:**\\n   - The `.bytes_stream()` method converts the response body into a `Stream` of `Bytes` chunks.\\n   - **Crucially**, this stream processes the response incrementally, allowing you to handle large files without high memory consumption.\\n\\n3. **Writing Chunks Directly to Disk:**\\n   - By iterating over `stream.next().await`, you handle each chunk as it arrives and immediately write it to the file.    \\n   - **`send().await?`**: Initiates the request and prepares to receive the response without buffering the entire body.\\n   - **`bytes_stream()`**: Explicitly creates a stream that processes the response body chunk by chunk, preventing full buffering.\\n\\nTherefore, **the provided code does indeed stream the response directly to disk without loading the entire response into memory**.\\n\\nFull code:\\n\\n```rust\\nuse reqwest::Client;\\nuse std::path::PathBuf;\\nuse tokio::fs::File;\\nuse tokio::io::AsyncWriteExt;\\nuse uuid::Uuid;\\nuse futures::StreamExt;\\n\\n/// Async function to download the repository as a ZIP file by streaming the response to disk.\\n/// Returns the path to the saved ZIP file.\\n///\\n/// # Arguments\\n///\\n/// * `owner` - The owner of the GitHub repository.\\n/// * `repo` - The name of the repository.\\n/// * `reference` - The branch, tag, or commit reference.\\n///\\n/// # Errors\\n///\\n/// Returns an error if the HTTP request fails or if writing to disk fails.\\npub async fn download_repo_as_zip(\\n    owner: &str,\\n    repo: &str,\\n    reference: &str,\\n) -> Result<PathBuf, Box<dyn std::error::Error>> {\\n    let url = format!(\\n        \\"https://api.github.com/repos/{owner}/{repo}/zipball/{reference}\\",\\n        owner = owner,\\n        repo = repo,\\n        reference = reference\\n    );\\n    let client = Client::new();\\n\\n    // GitHub requires a User-Agent header\\n    let resp = client\\n        .get(&url)\\n        .header(\\"User-Agent\\", \\"rust-zip-extractor\\")\\n        .send()\\n        .await?\\n        .error_for_status()?; // Ensure the request was successful\\n\\n    // Generate a unique filename using UUID\\n    let filename = format!(\\"{}_{}_{}.zip\\", owner, repo, Uuid::new_v4());\\n    let filepath = PathBuf::from(&filename);\\n\\n    // Create the file asynchronously\\n    let mut file = File::create(&filepath).await?;\\n\\n    // Convert the response body into a stream of bytes\\n    let mut stream = resp.bytes_stream();\\n\\n    // Iterate over the stream and write each chunk to the file\\n    while let Some(chunk) = stream.next().await {\\n        let data = chunk?; // Handle potential stream errors\\n        file.write_all(&data).await?;\\n    }\\n\\n    // Ensure all data is written to disk\\n    file.flush().await?;\\n\\n    Ok(filepath)\\n}\\n```"},{"id":"ch-01-ddia","metadata":{"permalink":"/byte_vault/blog/ch-01-ddia","source":"@site/blog/2025-01-05-ch-01-ddia.mdx","title":"Understanding DDIA - Chapter 01,02","description":"Explain the key differences between relational databases and document databases.","date":"2025-01-05T00:00:00.000Z","tags":[{"inline":false,"label":"Distributed Systems","permalink":"/byte_vault/blog/tags/dist-sys","description":"All things distributed systems"}],"readingTime":0.62,"hasTruncateMarker":false,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/blog/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"ch-01-ddia","title":"Understanding DDIA - Chapter 01,02","date":"2025-01-05T00:00:00.000Z","authors":["abeeshake"],"tags":["dist-sys"]},"unlisted":false,"prevItem":{"title":"Streaming HTTP to Disk","permalink":"/byte_vault/blog/streaming-http-to-disk"},"nextItem":{"title":"Deep Flattening in Rust - Using Recursive Types ","permalink":"/byte_vault/blog/deep-flattening-in-rust-using-recursive-types"}},"content":"import QSComponent, { Question, Solution } from \'../src/components/QSComponent\';\\n\\n<QSComponent>\\n  <Question>\\n    Explain the key differences between relational databases and document databases. \\n    When would you choose one over the other?\\n  </Question>\\n  <Solution>\\n        Here are the key differences between relational and document databases:\\n        \\n        1. **Schema**: \\n          - Relational: Rigid, predefined schema\\n          - Document: Flexible, schema-less design\\n\\n        2. **Data Model**:\\n          - Relational: Data normalized across tables\\n          - Document: Denormalized, nested documents\\n\\n        3. **Scalability**:\\n          - Relational: Vertical scaling primarily\\n          - Document: Easier horizontal scaling\\n\\n        Choose Relational when:\\n        - Data consistency is crucial\\n        - Complex queries and joins are needed\\n        - ACID compliance is required\\n        \\n        Choose Document when:\\n        - Schema flexibility is needed\\n        - Rapid prototyping\\n        - Handling large amounts of unstructured data\\n        - Horizontal scaling is a priority\\n  </Solution>\\n</QSComponent>"},{"id":"deep-flattening-in-rust-using-recursive-types","metadata":{"permalink":"/byte_vault/blog/deep-flattening-in-rust-using-recursive-types","source":"@site/blog/2024-12-31-rust-deep-flatten.mdx","title":"Deep Flattening in Rust - Using Recursive Types ","description":"Deep Flattening in Rust: A Recursive Adventure","date":"2024-12-31T00:00:00.000Z","tags":[{"inline":false,"label":"Rust","permalink":"/byte_vault/blog/tags/rust","description":"Rust lang"}],"readingTime":2.995,"hasTruncateMarker":true,"authors":[{"name":"Joel Medicala","url":"https://github.com/JoeruCodes","page":{"permalink":"/byte_vault/blog/authors/joel-medicala"},"socials":{"x":"https://x.com/JoeruCodes","github":"https://github.com/JoeruCodes"},"imageURL":"https://github.com/JoeruCodes.png","key":"joel-medicala"},{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/blog/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"deep-flattening-in-rust-using-recursive-types","title":"Deep Flattening in Rust - Using Recursive Types ","date":"2024-12-31T00:00:00.000Z","authors":["joel-medicala","abeeshake"],"tags":["rust"]},"unlisted":false,"prevItem":{"title":"Understanding DDIA - Chapter 01,02","permalink":"/byte_vault/blog/ch-01-ddia"},"nextItem":{"title":"Caddy Reverse Proxy Performance: 300% Boost with Unix Sockets","permalink":"/byte_vault/blog/how-to-solve-reverse-proxy-performance-issues-in-caddy-server-a-300-performance-boost-using-unix-sockets"}},"content":"### Deep Flattening in Rust: A Recursive Adventure\\n\\nFlattening nested data structures is a common problem in programming. However, flattening structures with an arbitrary depth\u2014like nested `Vec`s within `Vec`s\u2014can be tricky. Rust, with its strong type system and trait-based polymorphism, allows us to implement elegant solutions to such problems. In this post, we\'ll explore a recursive approach to deep flattening in Rust using traits, type inference, and iterators.\\n\\n#### The Goal\\n\\nGiven a deeply nested structure, such as:\\n\\n```rust\\nlet nested_vec = vec![\\n    vec![vec![1, 2, 3], vec![4, 5]],\\n    vec![vec![6], vec![7, 8, 9]],\\n];\\n```\\n\\nOur goal is to flatten it into:\\n\\n```rust\\nlet flattened = vec![1, 2, 3, 4, 5, 6, 7, 8, 9];\\n```\\n{/* truncate */}\\n\\nThe depth of nesting is not fixed\u2014it could be `Vec<Vec<Vec<T>>>`, `Vec<Vec<Vec<Vec<T>>>>`, or deeper.\\n\\n---\\n\\n### TL;DR: high level idea\\n\\nRust\u2019s iterators and traits allow us to create a type-safe, recursive implementation to handle deep flattening. The solution uses three key components:\\n\\n1. **The **Trait**: A recursive trait defining how to flatten iterators.\\n2. **Base and Recursive Implementations**: Separate implementations for handling the base case (non-nested items) and recursive case (nested items).\\n3. **A Wrapper Struct**: A helper type to simplify type inference.\\n\\n---\\n\\n### Implementation\\n\\nThe fun part lies in using Rust\'s types as in recursive way.\\n\\n#### The `DeepFlattenIteratorOf` Trait\\n\\nThis trait defines the recursive structure of our flattening logic:\\n\\n```rust\\npub trait DeepFlattenIteratorOf<Depth, T> {\\n    type DeepFlatten: Iterator<Item = T>;\\n    fn deep_flatten(this: Self) -> Self::DeepFlatten;\\n}\\n```\\n\\n- `Depth` tracks the nesting level.\\n- `T` is the type of the innermost element.\\n- `DeepFlatten` is the resulting iterator after flattening.\\n\\n#### Base Case: No Nesting\\n\\nThe base condition stops recursion when the structure is already flat:\\n\\n```rust\\nimpl<I: Iterator> DeepFlattenIteratorOf<(), I::Item> for I {\\n    type DeepFlatten = Self;\\n\\n    fn deep_flatten(this: Self) -> Self::DeepFlatten {\\n        this\\n    }\\n}\\n```\\n\\nHere, when `Depth` is `()`, no further flattening is needed, and the iterator is returned as-is.\\n\\n#### Recursive Case: Flatten Nested Items\\n\\nFor nested structures, the recursion continues until reaching the base case:\\n\\n```rust\\nimpl<Depth, I, T> DeepFlattenIteratorOf<(Depth,), T> for I\\nwhere\\n    Flatten<I>: DeepFlattenIteratorOf<Depth, T>,\\n    I: Iterator,\\n    I::Item: IntoIterator,\\n{\\n    type DeepFlatten = <Flatten<I> as DeepFlattenIteratorOf<Depth, T>>::DeepFlatten;\\n\\n    fn deep_flatten(this: Self) -> Self::DeepFlatten {\\n        DeepFlattenIteratorOf::deep_flatten(this.flatten())\\n    }\\n}\\n```\\n\\n- `Flatten<I>` handles one level of flattening.\\n- The recursion continues until it reaches the base case.\\n\\n#### Wrapper Struct for Type Inference\\n\\nThe `DeepFlatten` struct simplifies type inference by wrapping the recursive logic:\\n\\n```rust\\npub struct DeepFlatten<Depth, I, T>\\nwhere\\n    I: DeepFlattenIteratorOf<Depth, T>,\\n{\\n    inner: I::DeepFlatten,\\n}\\n\\nimpl<I: Iterator> DeepFlattenExt for I {}\\n```\\n\\nThis allows users to call the `.deep_flatten()` method directly:\\n\\n```rust\\npub trait DeepFlattenExt: Iterator + Sized {\\n    fn deep_flatten<Depth, T>(self) -> DeepFlatten<Depth, Self, T>\\n    where\\n        Self: DeepFlattenIteratorOf<Depth, T>,\\n    {\\n        DeepFlatten {\\n            inner: DeepFlattenIteratorOf::deep_flatten(self),\\n        }\\n    }\\n}\\n```\\n\\n#### Iterator Implementation for `DeepFlatten`\\n\\nFinally, the iterator implementation allows seamless iteration over flattened items:\\n\\n```rust\\nimpl<Depth, I, T> Iterator for DeepFlatten<Depth, I, T>\\nwhere\\n    I: DeepFlattenIteratorOf<Depth, T>,\\n{\\n    type Item = T;\\n\\n    fn next(&mut self) -> Option<Self::Item> {\\n        self.inner.next()\\n    }\\n}\\n```\\n\\n---\\n\\n### Example Usage\\n\\nHere\u2019s how you can use the `deep_flatten` method to flatten nested structures:\\n\\n```rust\\nfn main() {\\n    let nested_vec = vec![\\n        vec![vec![1, 2, 3], vec![4, 5]],\\n        vec![vec![6], vec![7, 8, 9]],\\n    ];\\n\\n    let flattened: Vec<i32> = nested_vec.into_iter().deep_flatten().collect();\\n\\n    assert_eq!(flattened, vec![1, 2, 3, 4, 5, 6, 7, 8, 9]);\\n\\n    println!(\\"Flattened result: {:?}\\", flattened);\\n}\\n```\\n\\n---\\n\\nThis code gist was prepared by Joel is [available on rust playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2021&gist=dbd26d3c4e89abbf50cde86dec296cd7)!\\n\\nThanks Joel once again for bringing light to this pattern! \\nThat\'s a wrap for this year! \\n\\nSee you in next year!"},{"id":"how-to-solve-reverse-proxy-performance-issues-in-caddy-server-a-300-performance-boost-using-unix-sockets","metadata":{"permalink":"/byte_vault/blog/how-to-solve-reverse-proxy-performance-issues-in-caddy-server-a-300-performance-boost-using-unix-sockets","source":"@site/blog/2024-12-23-caddy-performance.mdx","title":"Caddy Reverse Proxy Performance: 300% Boost with Unix Sockets","description":"A recent GitHub issue  #6751  in the Caddy server repository revealed an interesting performance bottleneck when using multiple layers of reverse proxying. Here\'s what was discovered and how it was resolved.","date":"2024-12-23T00:00:00.000Z","tags":[{"inline":false,"label":"Performance","permalink":"/byte_vault/blog/tags/performance","description":"Blog posts related to improving and understanding performance."},{"inline":false,"label":"Caddy","permalink":"/byte_vault/blog/tags/caddy","description":"caddy config and tuning"}],"readingTime":3.965,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/blog/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"how-to-solve-reverse-proxy-performance-issues-in-caddy-server-a-300-performance-boost-using-unix-sockets","title":"Caddy Reverse Proxy Performance: 300% Boost with Unix Sockets","date":"2024-12-23T00:00:00.000Z","authors":["abeeshake"],"tags":["performance","caddy"]},"unlisted":false,"prevItem":{"title":"Deep Flattening in Rust - Using Recursive Types ","permalink":"/byte_vault/blog/deep-flattening-in-rust-using-recursive-types"},"nextItem":{"title":"1brc - same tricks across languages","permalink":"/byte_vault/blog/1brc-same-tricks-across-languages"}},"content":"A recent GitHub issue  [#6751](https://github.com/caddyserver/caddy/issues/6751)  in the Caddy server repository revealed an interesting performance bottleneck when using multiple layers of reverse proxying. Here\'s what was discovered and how it was resolved.\\n\\n{/* truncate */}\\n\\n## The Problem\\n\\nA user reported significant performance degradation when implementing multiple layers of reverse proxies in Caddy v2.8.4. The setup consisted of a chain of reverse proxies:\\n\\n- Port 8081: Serving static files\\n- Port 8082: Proxying to 8081\\n- Port 8083: Proxying to 8082\\n- Port 8084: Proxying to 8083\\n\\nWhen testing with a 1000MB file download, the performance metrics showed a clear pattern of degradation:\\n\\n### Multi-Threading Performance Impact\\n\\n- Direct file server (8081): ~300 Mbps with 5 threads\\n- First proxy layer (8082): ~60 Mbps with 5 threads\\n- Second proxy layer (8083): ~16 Mbps with 5 threads\\n- Third proxy layer (8084): ~16 Mbps with 5 threads\\n\\nWhat made this particularly interesting was that the server\'s CPU usage remained surprisingly low (1-5%), suggesting that the bottleneck wasn\'t in processing power.\\n\\n## The Investigation\\n\\nThe investigation, led by Caddy maintainers including Matt Holt, involved:\\n\\n1. Gathering system metrics\\n2. Analyzing CPU and memory profiles\\n3. Testing different network configurations\\n4. Examining kernel settings\\n\\n### Table 1: System Metrics\\n\\n\\n| **Commands**                                 | **Why It Is Relevant to Debugging**                                                                                               | **Output and Conclusion**                                                                                                                                                                                                                                                |\\n| -------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| `ulimit -a`                                  | Checks system limits such as maximum number of open files and other resource constraints that could impact performance.           | No bottlenecks identified in file descriptors or resource limits.                                                                                                                                                                                                        |\\n| `sysctl -p`                                  | Confirms network-related kernel parameters such as buffer sizes, default queuing discipline, and TCP congestion control settings. | <br/>`net.core.rmem_max = 2097152`<br/>`net.core.wmem_max = 2097152`<br/>`net.core.default_qdisc = fq`<br/>`net.ipv4.tcp_congestion_control = bbr`<br/><br/> Settings were optimized for high-speed networking.<br />TCP congestion control was correctly set to`bbr`. |\\n| General hardware specs (CPU, RAM, NIC, etc.) | baseline hardware information                                                                                                     | Verified adequate resources (1 Core Ryzen 5950X, 1024MB RAM, 10Gbps NIC). No resource-related constraints.                                                                                                                                                               |\\n\\n---\\n\\n### Table 2: Profile Analysis\\n\\n\\n| **Commands**                                                  | **Why It Is Relevant to Debugging**                                                                | **Output and Conclusion**                                                                          |\\n| --------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\\n| Attempted to collect goroutine profiles                       | Helps identify bottlenecks or inefficiencies in goroutines that may be causing performance issues. | Could not identify significant bottlenecks in goroutines.                                          |\\n| Accessed CPU Profile via browser                              | Provides CPU usage details to determine if high CPU usage is a factor affecting performance.       | No high CPU usage detected. CPU load was between 1-5%.                                             |\\n| `wget http://127.0.0.1:2019/debug/pprof/profile?seconds=1000` | Downloads detailed CPU profiles for offline analysis.                                              | Profiles downloaded successfully. Further analysis confirmed no CPU bottlenecks or inefficiencies. |\\n| Collected heap profiles                                       | Helps analyze memory usage and potential leaks in the application.                                 | Memory usage was within acceptable limits, with no indication of memory leaks.                     |\\n\\n---\\n\\n### Table 3: Network Testing\\n\\n\\n| **Commands**                                                                           | **Why It Is Relevant to Debugging**                                                          | **Output and Conclusion**                                               |\\n| -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |\\n| Tests from multiple locations (Singapore, Los Angeles, Seoul)                          | Evaluates network performance across different regions to identify geographical bottlenecks. | Performance was consistent across all regions.                          |\\n| Tests with different file sizes (100MiB, 1000MiB)                                      | Determines if performance issues are related to file size or payload.                        | No significant performance variance with different file sizes.          |\\n| `curl -o /dev/null http://host.domain:port/1000MiB`                                    | Single-threaded test evaluates download performance under minimal concurrency.               | acceptable network speed                                                |\\n| `echo 1 1 1 1 1 \\\\| xargs -n1 -P5 curl -s -o /dev/null http://host.domain:port/1000MiB` |                                                                                              | Multi-threaded test assesses network performance under concurrent load. |\\n\\n\\n---\\n\\n### Table 4: Kernel Analysis\\n\\n\\n| **Commands**                                       | **Why It Is Relevant to Debugging**                                                                  | **Output and Conclusion**                                         |\\n| -------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |\\n| Checked systemd service file settings              | Confirms that the maximum number of open files is sufficient for high-concurrency workloads.         | Verified`LimitNOFILE=1048576`. No issues found.                   |\\n\\n\\n## The Solution\\n\\nThe breakthrough came when testing with Unix sockets instead of TCP connections. By modifying the Caddyfile to use Unix sockets for inter-process communication, the performance issues were completely resolved. Here\'s what the optimized configuration looked like:\\n\\n```\\n:8081 {\\n    bind 0.0.0.0 unix//dev/shm/8081.sock\\n    file_server browse\\n    root * /opt/www\\n}\\n\\n:8082 {\\n    bind 0.0.0.0 unix//dev/shm/8082.sock\\n    reverse_proxy unix//dev/shm/8081.sock\\n}\\n\\n:8083 {\\n    bind 0.0.0.0 unix//dev/shm/8083.sock\\n    reverse_proxy unix//dev/shm/8082.sock\\n}\\n\\n:8084 {\\n    reverse_proxy unix//dev/shm/8083.sock\\n}\\n```\\n\\n## Key Takeaways\\n\\n1. TCP connection overhead can significantly impact performance in multi-layer reverse proxy setups\\n2. Unix sockets provide a more efficient alternative for local inter-process communication\\n3. Low CPU usage doesn\'t always mean optimal performance - network stack overhead can be the bottleneck\\n4. When dealing with multiple local reverse proxies, consider using Unix sockets instead of TCP connections"},{"id":"1brc-same-tricks-across-languages","metadata":{"permalink":"/byte_vault/blog/1brc-same-tricks-across-languages","source":"@site/blog/2024-12-22-1brc-common-learnings.mdx","title":"1brc - same tricks across languages","description":"The 1 Billion Row Challenge (1BRC) is a programming challenge focused on processing a large dataset of temperature measurements. If you\'re unfamiliar with it, you can learn more from these resources: 1 and 2.","date":"2024-12-22T00:00:00.000Z","tags":[{"inline":false,"label":"Performance","permalink":"/byte_vault/blog/tags/performance","description":"Blog posts related to improving and understanding performance."}],"readingTime":2.16,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/blog/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"1brc-same-tricks-across-languages","title":"1brc - same tricks across languages","date":"2024-12-22T00:00:00.000Z","authors":["abeeshake"],"tags":["performance"]},"unlisted":false,"prevItem":{"title":"Caddy Reverse Proxy Performance: 300% Boost with Unix Sockets","permalink":"/byte_vault/blog/how-to-solve-reverse-proxy-performance-issues-in-caddy-server-a-300-performance-boost-using-unix-sockets"}},"content":"The 1 Billion Row Challenge (1BRC) is a programming challenge focused on processing a large dataset of temperature measurements. If you\'re unfamiliar with it, you can learn more from these resources: [1](https://github.com/gunnarmorling/1brc) and [2](https://www.morling.dev/blog/1brc-results-are-in/).\\n\\nThis is a cheatsheet  of optimisations done for 1brc challenges. It tries to summarise and put the optimisations in perspective.\\n\\n{/* truncate */}\\n\\n### Data encoding / parsing\\n\\n\\n| Trick                                         | Outcome                                                                                     | Note                                                                                                                                                 |\\n| :---------------------------------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| converting contents of file as string: don\'t! | [~10% perf](https://github.com/gunnarmorling/1brc/discussions/57#discussioncomment-8153186) | Don\'t convert the contents of the file to`String`. Simply process raw bytes.                                                                         |\\n| parsing float values<br />                    |                                                                                             | parsing after assuming only one place after decimal.                                                                                                 |\\n| parsing integer                               |                                                                                             | [Branchless Programming via Bit Manipulation](https://youtu.be/EFXxXFHpS0M?t=1255)<br /> this is not generalizable                                   |\\n| parsing the city name (finding`;` separator)  |                                                                                             | If the data looks like`Tokio;13.4` and we want to find `;`<br/><br/>using 8 operations, you can find `;`.<br /><br /><br />SWAR = SIMD as a register |\\n\\n### Reading files from disk\\n\\n\\n| Trick                                                                                                                                  | Outcome                                                                                     | Note                                                                                                                                                                                                                    |\\n| ---------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Reading in chunks[golang](https://www.bytesizego.com/blog/one-billion-row-challenge-go#:~:text=Reading%20file%3A%20Read%20in%20chunks) | [~10% perf](https://github.com/gunnarmorling/1brc/discussions/57#discussioncomment-8153186) |                                                                                                                                                                                                                         |\\n| mmap                                                                                                                                   |                                                                                             | [mmap the input file. Split input based on core count and run it with a thread per core.](https://github.com/gunnarmorling/1brc/discussions/57#discussioncomment-8041416)<br/><br/>mmap is an unsafe operation in Rust. |\\n\\n### Float handling\\n\\n\\n| Trick                                        | Outcome                                                                                                         | Notes |\\n| ---------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | ------- |\\n| Don\'t do floating point operations. Use int. | [20% speed gains](https://github.com/gunnarmorling/1brc/discussions/57#discussioncomment-8024568)               |       |\\n| Use fixed size integers instead of float     | [~10% gain in golang](https://benhoyt.com/writings/go-1brc/#:~:text=Solution%204%3A%20fixed%20point%20integers) |       |\\n\\n### The Hashmap - simpler hash function\\n\\n\\n| Trick                                                                                                                                                                                                                        | Outcome                                                                                                                                                                                                                        | Note                                                                                                                                                                                                                                                                                                                                                                                        |\\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [Fixed Size hash table 10k](https://github.com/gunnarmorling/1brc/discussions/57#:~:text=Treat%20the%20first%20eight%20bytes%20of%20the%20name%20as%20a%20hash%20key%20into%20a%20fixed%20size%2010k%20item%20hash%20table.) | [~40% gain - the custom hash table cuts down the time from 41.3 seconds to 22.1s.](https://benhoyt.com/writings/go-1brc/#:~:text=the%20custom%20hash%20table%20cuts%20down%20the%20time%20from%2041.3%20seconds%20to%2022.1s.) | How to resolve collisions?<br /><br />- Find first unoccupied slot<br />- <br />[if hash collide, goto next empty slot algorithm](https://benhoyt.com/writings/go-****1brc/#:~:text=It%E2%80%99s%20a%20simple%20implementation%20that%20uses%20the%20FNV%2D1a%20hash%20algorithm%20with%20linear%20probing%3A%20if%20there%E2%80%99s%20a%20collision%2C%20use%20the%20next%20empty%20slot.) |\\n| Hash key - Integer, not String                                                                                                                                                                                               | [4x gain in a different competition](https://youtu.be/SVw9nKfVPx4?t=501)                                                                                                                                                       | the time is spent in`Hashmap.get`.<br /> Not in 1brc, but in [other context](https://www.youtube.com/watch?v=SVw9nKfVPx4&t=501s), Hashmap.get was the bottleneck. hence this optimisation makes a lot of difference there, not in 1brc.                                                                                                                                                     |\\n\\n### With OS threads and parallelism - go brr!\\n\\n\\n| Trick                                                        | Outcome                                                                                                                                                                                                                                  | Notes                                                     |\\n| -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------- |\\n| address the go routine + rust tokio task + erlang processes? | [~ 4-6x gains](https://benhoyt.com/writings/go-1brc/#:~:text=Processing%20the%20input%20file%20in%20parallel%20provides%20a%20huge%20win%20over%20r1%2C%20taking%20the%20time%20from%201%20minute%2045%20seconds%20to%2022.6%20seconds.) | biggest gains are obtained by utilizing OS threads fully! |"}]}}')}}]);