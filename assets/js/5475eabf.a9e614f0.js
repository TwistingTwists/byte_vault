"use strict";(self.webpackChunkbytevault=self.webpackChunkbytevault||[]).push([[4882],{70:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"database-isolation-visualised-dirty-writes","metadata":{"permalink":"/byte_vault/database-isolation-visualised-dirty-writes","source":"@site/blog/2025-04-22-db-isolation-002.mdx","title":"Database Isolation (dirty writes)","description":"Understanding Dirty Writes: A Database Isolation Problem","date":"2025-04-22T00:00:00.000Z","tags":[{"inline":true,"label":"databases","permalink":"/byte_vault/tags/databases"},{"inline":true,"label":"transactions","permalink":"/byte_vault/tags/transactions"}],"readingTime":2.085,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"database-isolation-visualised-dirty-writes","title":"Database Isolation (dirty writes)","date":"2025-04-22T00:00:00.000Z","authors":["abeeshake"],"tags":["databases","transactions"],"draft":false},"unlisted":false,"nextItem":{"title":"Database Isolation (dirty reads)","permalink":"/byte_vault/database-isolation-visualised-dirty-reads"}},"content":"import TOCInline from \'@theme/TOCInline\';\\n\\n<TOCInline toc={toc} />\\n\\nimport DirtyWritesTimeline from \\"../src/components/Database/DirtyWritesTimeline\\";\\n\\n## Understanding Dirty Writes: A Database Isolation Problem\\n\\n### What is a Dirty Write?\\n\\nA **dirty write** occurs when multiple transactions try to update the same data concurrently without proper isolation,\\npotentially causing one transaction to overwrite changes made by another uncommitted transaction.\\n\\nThis can lead to inconsistent or incorrect results in applications, especially in systems where multiple users or processes are accessing and modifying the data concurrently.\\n\\n{/* truncate */}\\n---\\n\\n### Example Scenario\\n\\nLet\'s walk through a concrete example to see how a dirty write can happen.\\n\\n#### Initial Database State\\n\\nSuppose we have a simple database table:\\n\\n| key  | value    |\\n| ---- | -------- |\\n| name | abhishek |\\n| age  | 42       |\\n\\nTwo transactions, **T1** and **T2**, will interact with the `name` row.\\n\\n---\\n\\n### Step-by-Step: Dirty Write in Action\\n\\n#### 1. T1 Reads the Original Value\\n\\n- **T1** starts and reads the value of `name`.\\n- It sees:  \\n  `name = \'abhishek\'` (the committed value).\\n\\n#### 2. T2 Updates the Value (But Doesn\'t Commit)\\n\\n- **T2** starts and updates `name` to `\'john\'`.\\n- This change is **not yet committed** to the database.\\n- Other transactions should ideally not see this uncommitted value.\\n\\n#### 3. T1 Reads Again (Dirty Write!)\\n\\n- **T1** reads the value of `name` again.\\n- **Because the database isolation level is low (e.g., READ UNCOMMITTED), T1 sees `name = \'john\'`**, even though T2 hasn\'t committed.\\n- This is a **dirty write**: T1 is reading data that might be rolled back.\\n\\n#### 4. T2 Commits\\n\\n- **T2** now commits its change.\\n- `name = \'john\'` is now the official, permanent value.\\n\\n#### 5. T1 Commits\\n\\n- **T1** commits, possibly basing its logic on the dirty value it read earlier.\\n\\n---\\n\\n## Why is This a Problem?\\n\\nIf **T2** had rolled back instead of committing, **T1** would have read a value that never officially existed in the database. This can cause application bugs, data corruption, or security issues.\\n\\n---\\n\\n## Visualizing the Dirty Write\\n\\nThe animation below demonstrates this scenario step-by-step. You can see how the database and transactions interact, and exactly when the dirty write occurs.\\n\\n<DirtyWritesTimeline />\\n---\\n\\n## Conclusion\\n\\nDirty writes are a classic example of why database isolation levels matter. By default, most production databases avoid dirty writes by using stricter isolation (like READ COMMITTED or higher). However, understanding this scenario helps you appreciate the trade-offs between performance and consistency in concurrent systems."},{"id":"database-isolation-visualised-dirty-reads","metadata":{"permalink":"/byte_vault/database-isolation-visualised-dirty-reads","source":"@site/blog/2025-04-21-db-isolation-001.mdx","title":"Database Isolation (dirty reads)","description":"Understanding Dirty Reads: A Database Isolation Problem","date":"2025-04-21T00:00:00.000Z","tags":[{"inline":true,"label":"databases","permalink":"/byte_vault/tags/databases"},{"inline":true,"label":"transactions","permalink":"/byte_vault/tags/transactions"}],"readingTime":2.12,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"database-isolation-visualised-dirty-reads","title":"Database Isolation (dirty reads)","date":"2025-04-21T00:00:00.000Z","authors":["abeeshake"],"tags":["databases","transactions"],"draft":false},"unlisted":false,"prevItem":{"title":"Database Isolation (dirty writes)","permalink":"/byte_vault/database-isolation-visualised-dirty-writes"},"nextItem":{"title":"Understanding Eventloops (Tokio Internals)","permalink":"/byte_vault/tokio-internals-visualised"}},"content":"import TOCInline from \'@theme/TOCInline\';\\n\\n<TOCInline toc={toc} />\\n\\nimport DirtyReadsTimeline from \\"../src/components/Database/DirtyReadsTimeline\\";\\n\\n## Understanding Dirty Reads: A Database Isolation Problem\\n\\n### What is a Dirty Read?\\n\\nA **dirty read** occurs when a transaction reads data that has been written by another transaction but not yet committed. If the writing transaction rolls back, the data read by the first transaction becomes invalid\u2014hence, \\"dirty.\\"\\n\\nThis can lead to inconsistent or incorrect results in applications, especially in systems where multiple users or processes are accessing and modifying the data concurrently.\\n\\n{/* truncate */}\\n---\\n\\n### Example Scenario\\n\\nLet\'s walk through a concrete example to see how a dirty read can happen.\\n\\n#### Initial Database State\\n\\nSuppose we have a simple database table:\\n\\n| key  | value    |\\n| ---- | -------- |\\n| name | abhishek |\\n| age  | 42       |\\n\\nTwo transactions, **T1** and **T2**, will interact with the `name` row.\\n\\n---\\n\\n### Step-by-Step: Dirty Read in Action\\n\\n#### 1. T1 Reads the Original Value\\n\\n- **T1** starts and reads the value of `name`.\\n- It sees:  \\n  `name = \'abhishek\'` (the committed value).\\n\\n#### 2. T2 Updates the Value (But Doesn\'t Commit)\\n\\n- **T2** starts and updates `name` to `\'john\'`.\\n- This change is **not yet committed** to the database.\\n- Other transactions should ideally not see this uncommitted value.\\n\\n#### 3. T1 Reads Again (Dirty Read!)\\n\\n- **T1** reads the value of `name` again.\\n- **Because the database isolation level is low (e.g., READ UNCOMMITTED), T1 sees `name = \'john\'`**, even though T2 hasn\'t committed.\\n- This is a **dirty read**: T1 is reading data that might be rolled back.\\n\\n#### 4. T2 Commits\\n\\n- **T2** now commits its change.\\n- `name = \'john\'` is now the official, permanent value.\\n\\n#### 5. T1 Commits\\n\\n- **T1** commits, possibly basing its logic on the dirty value it read earlier.\\n\\n---\\n\\n## Why is This a Problem?\\n\\nIf **T2** had rolled back instead of committing, **T1** would have read a value that never officially existed in the database. This can cause application bugs, data corruption, or security issues.\\n\\n---\\n\\n## Visualizing the Dirty Read\\n\\nThe animation below demonstrates this scenario step-by-step. You can see how the database and transactions interact, and exactly when the dirty read occurs.\\n\\n<DirtyReadsTimeline />\\n---\\n\\n## Conclusion\\n\\nDirty reads are a classic example of why database isolation levels matter. By default, most production databases avoid dirty reads by using stricter isolation (like READ COMMITTED or higher). However, understanding this scenario helps you appreciate the trade-offs between performance and consistency in concurrent systems."},{"id":"tokio-internals-visualised","metadata":{"permalink":"/byte_vault/tokio-internals-visualised","source":"@site/blog/2025-04-19-tokio-internals.mdx","title":"Understanding Eventloops (Tokio Internals)","description":"Prelude","date":"2025-04-19T00:00:00.000Z","tags":[{"inline":false,"label":"Rust","permalink":"/byte_vault/tags/rust","description":"Rust lang"},{"inline":false,"label":"Tokio","permalink":"/byte_vault/tags/tokio","description":"Tokio async runtime"},{"inline":false,"label":"Async","permalink":"/byte_vault/tags/async","description":"Asynchronous programming"},{"inline":false,"label":"Event Loop","permalink":"/byte_vault/tags/eventloop","description":"Event loop"}],"readingTime":2.805,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"tokio-internals-visualised","title":"Understanding Eventloops (Tokio Internals)","date":"2025-04-19T00:00:00.000Z","authors":["abeeshake"],"tags":["rust","tokio","async","eventloop"],"draft":false},"unlisted":false,"prevItem":{"title":"Database Isolation (dirty reads)","permalink":"/byte_vault/database-isolation-visualised-dirty-reads"},"nextItem":{"title":"Connection Pooling - in Depth","permalink":"/byte_vault/connection-pooling-in-depth"}},"content":"import ThreadResourceHtopVisualization from \\"../src/components/Tokio/T02ThreadAnimation\\";\\nimport LearningObjective from \\"../src/components/LearningObjective\\";\\nimport SimpleAccordion from \\"../src/components/SimpleAccordion\\";\\nimport Insight from \\"../src/components/Insight\\"; // Assuming Insight is imported from here\\n\\n\\n\\nimport TOCInline from \'@theme/TOCInline\';\\n\\n<TOCInline toc={toc} />\\n\\n\\n## Prelude\\nThis is the first post in a four part series that will provide an understanding of the mechanics behind the Tokio runtime in Rust. This post focuses on the challenges in a multi-threaded event loop that force us to think of async runtimes like Tokio.\\n\\nIndex of the four part series:\\n\\n1. Visualizing Tokio Internals: Part I - Multi-Threaded Event Loop / Server\\n2. Visualizing Tokio Internals: Part II - Reactor\\n3. Visualizing Tokio Internals: Part III - Wakers\\n4. Visualizing Tokio Internals: Part IV - Executors\\n\\n## Multi-Threaded Event Loop / Server\\n\\nWhat challenges in a multi-threaded event loop force us to think of async runtimes like Tokio?\\n\\n\\n## Phase 0: The Problem\\n\\n<LearningObjective \\n  question=\\"Why do we need async runtimes like Tokio?\\"\\n  id=\\"why-tokio\\"\\n>\\n  - **Resource Efficiency:** Traditional thread-per-connection models waste system resources\\n  - **Scalability:** Async enables handling thousands of connections with minimal overhead\\n  - **Performance:** Event-driven architecture reduces context switching and memory usage\\n  - **Cost-Effective:** Better resource utilization means lower infrastructure costs\\n</LearningObjective>\\n\\n{/* truncate */}\\n\\nModern applications, especially network services, need to handle many things concurrently. Imagine a web server handling thousands of client connections simultaneously.\\n\\nA naive approach is to dedicate one Operating System (OS) thread to each connection. Let\'s see why this doesn\'t scale well.\\n\\n### The Thread-Per-Connection Resource Drain\\n\\nThe visualization below shows resource consumption (CPU/Memory) and throughput limits of a blocking thread-per-connection model.\\n\\n\\n<SimpleAccordion \\n  summary=\\"How a thread-per-connection server behaves as load increases\\"\\n  id=\\"thread-per-connection\\"\\n  className=\\"mb-4\\"\\n>\\n  **Description:**\\n\\n  Imagine a dashboard resembling `htop` or Task Manager:\\n\\n  1. **CPU Usage:** Bars representing individual CPU cores.\\n  2. **Memory Usage:** A single bar showing total RAM consumption.\\n  3. **Active Threads:** A counter or list showing running OS threads.\\n  4. **Requests/Second:** A throughput meter.\\n  5. **Incoming Requests Queue:** A visual queue of pending connections.\\n\\n  **Simulation:**\\n\\n  - **Start:** The server starts. CPU/Memory usage is low. Throughput is 0. Few base threads exist.\\n  - **Low Load:** Simulate a few incoming connections (~10). For each, a new OS thread is created.\\n    - *Visual:* Active Threads count increases slightly. Memory usage ticks up slightly. CPU usage might blip as threads start but stays relatively low if connections are mostly idle. Throughput matches the request rate.\\n  - **High Load:** Simulate hundreds or thousands of incoming connections. Many connections involve waiting for network I/O (reading request body, waiting for database, sending response).\\n    - *Visual:*\\n      - **Active Threads:** The count explodes. Each thread requires kernel resources and its own stack (~MBs).\\n      - **Memory Usage:** The Memory bar shoots up dramatically, potentially hitting system limits.\\n      - **CPU Usage:** CPU bars likely thrash. Even if threads are mostly *waiting* (blocked on I/O), the OS spends significant time *context switching* between them. This is overhead, not useful work.\\n      - **Requests Queue:** The incoming requests queue grows rapidly because threads are created, but many quickly block on I/O. The server struggles to accept new connections.\\n      - **Requests/Second:** The throughput meter hits a plateau far below the incoming request rate, possibly even decreasing as context-switching overhead dominates.\\n</SimpleAccordion>\\n\\n<ThreadResourceHtopVisualization className=\\"mt-8\\" />\\n\\n<Insight>\\n  We need a way to handle multiple waiting tasks concurrently without needing a dedicated OS thread for each one <em>while it\'s waiting</em>. This leads to asynchronous programming.\\n</Insight>"},{"id":"connection-pooling-in-depth","metadata":{"permalink":"/byte_vault/connection-pooling-in-depth","source":"@site/blog/2025-03-13-connection-pooling.mdx","title":"Connection Pooling - in Depth","description":"Here\u2019s a Markdown table that maps real-life reverse proxy scenarios to recommended TCP tuning parameters for optimal performance and security:","date":"2025-03-13T00:00:00.000Z","tags":[{"inline":false,"label":"Connection","permalink":"/byte_vault/tags/connection","description":"Network connections"},{"inline":false,"label":"Database","permalink":"/byte_vault/tags/database","description":"Database systems"},{"inline":false,"label":"Network","permalink":"/byte_vault/tags/network","description":"Networking concepts"}],"readingTime":2.115,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"connection-pooling-in-depth","title":"Connection Pooling - in Depth","date":"2025-03-13T00:00:00.000Z","authors":["abeeshake"],"tags":["connection","database","network"],"draft":false},"unlisted":false,"prevItem":{"title":"Understanding Eventloops (Tokio Internals)","permalink":"/byte_vault/tokio-internals-visualised"},"nextItem":{"title":"Reliable Structured Outputs with LLMs","permalink":"/byte_vault/reliable-structured-outputs-with-llms"}},"content":"Here\u2019s a **Markdown table** that maps **real-life reverse proxy scenarios** to recommended **TCP tuning parameters** for optimal performance and security:\\n\\n### \u2705 **Legend (Quick Reference)**\\n| **Parameter**                  | **Purpose**                                  |\\n|-------------------------------|----------------------------------------------|\\n| `tcp_fin_timeout`              | How long to keep closing connection in FIN state. |\\n| `tcp_keepalive_time`           | Idle time before sending first keep-alive probe. |\\n| `tcp_keepalive_intvl`          | Interval between successive keep-alive probes. |\\n| `tcp_keepalive_probes`         | Number of probes before dropping connection.  |\\n| `tcp_retries2`                 | Max TCP retransmissions before giving up.    |\\n\\n{/* truncate */}\\n\\n---\\n\\n### \u2705 **TCP Tuning Recommendations for Reverse Proxy - Real Life Scenarios**\\n\\n| **Scenario**                                  | **tcp_fin_timeout** | **tcp_keepalive_time** | **tcp_keepalive_intvl** | **tcp_keepalive_probes** | **tcp_retries2** | **Reasoning & Trade-offs**                                                                                   |\\n|------------------------------------------------|---------------------|------------------------|-------------------------|--------------------------|------------------|--------------------------------------------------------------------------------------------------------------|\\n| **Public API Gateway (high concurrent clients)** | `15`                | `30`                   | `10`                    | `3`                      | `5`              | Quick cleanup of dead/idle connections to save resources, while allowing short keep-alives for API clients.  |\\n| **Internal microservices (low latency, stable network)** | `10`                | `60`                   | `20`                    | `3`                      | `3`              | Fast connection recycling, rare need for keep-alives due to low latency, prioritizing efficiency.             |\\n| **Mobile-heavy client traffic (prone to network drops)** | `30`                | `120`                  | `20`                    | `5`                      | `7`              | More lenient timeouts to account for intermittent mobile network instability; avoid prematurely dropping clients. |\\n| **WebSocket / long-lived connections (chat apps, gaming)** | `60`                | `300`                  | `60`                    | `5`                      | `8`              | Allow long idle connections; keep-alives to detect dead connections without cutting active clients abruptly.  |\\n| **DDoS-prone public proxy (security-focused)**  | `5`                 | `30`                   | `5`                     | `2`                      | `3`              | Aggressive timeouts to prevent resource exhaustion; fast cleanup of potentially malicious connections.         |\\n| **IoT Device Communication (sporadic, unstable)** | `30`                | `180`                  | `30`                    | `4`                      | `6`              | Longer keep-alives to maintain connection with low-power devices, balanced with cleanup to avoid idle hangs.  |\\n| **Slow clients behind proxies (corporate clients, satellite)** | `20`                | `150`                  | `30`                    | `4`                      | `6`              | Moderate timeouts to handle slow networks without dropping legitimate users.                                  |\\n\\n---\\n\\n### \u2699\ufe0f **Notes:**\\n- **Lower timeouts**: Free up resources quickly, but risk dropping slow/legit connections.\\n- **Higher timeouts**: Improve user experience over slow networks but consume more resources.\\n- **Keep-alive settings**: Essential for long-lived or idle connections to detect dead peers.\\n- **Retries**: Trade-off between network resilience and resource use.\\n\\n---\\n\\nIf you want, I can prepare a **`sysctl.conf` file snippet** based on any of these scenarios for direct use. Let me know! \ud83d\ude80\\n\\n\\n\\n----\\n\\nsource: https://github.com/brettwooldridge/HikariCP/wiki/Down-the-Rabbit-Hole\\n\\n----"},{"id":"reliable-structured-outputs-with-llms","metadata":{"permalink":"/byte_vault/reliable-structured-outputs-with-llms","source":"@site/blog/2025-03-01-parsing-json-from-llm.mdx","title":"Reliable Structured Outputs with LLMs","description":"Ensuring Deterministic Outputs from LLMs","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"JSON Parser","permalink":"/byte_vault/tags/json-parser","description":"Parsing JSON"},{"inline":false,"label":"LLM","permalink":"/byte_vault/tags/llm","description":"Large Language Models"}],"readingTime":1.95,"hasTruncateMarker":false,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"reliable-structured-outputs-with-llms","title":"Reliable Structured Outputs with LLMs","date":"2025-03-01T00:00:00.000Z","draft":false,"authors":["abeeshake"],"tags":["json-parser","llm"]},"unlisted":false,"prevItem":{"title":"Connection Pooling - in Depth","permalink":"/byte_vault/connection-pooling-in-depth"},"nextItem":{"title":"String interning in Rust","permalink":"/byte_vault/string-interning-rust"}},"content":"### Ensuring Deterministic Outputs from LLMs\\n\\nThere are several strategies to obtain structured outputs from LLMs.\\n\\nIn Python, libraries such as Pydantic and Instructor facilitate structured output via JSON schema-based tool invocation. If you have the capability to host your own model, sglang is a viable option.\\n\\nPydantic validators are highly effective, provided that the input is in the form of a valid JSON string.\\n\\nLet\'s see by example. For starters, here is the schema we want to parse.\\n\\n```python\\nfrom pydantic import BaseModel, ValidationError\\n\\nclass User(BaseModel):\\n    id: int\\n    name: str\\n    email: str\\n    active: bool = True  # default value\\n\\n# JSON representation of the data\\njson_data = \'\'\'\\n{\\n    \\"id\\": 123,\\n    \\"name\\": \\"Alice\\",\\n    \\"email\\": \\"alice@example.com\\"\\n}\\n\'\'\'\\n\\ntry:\\n     # Directly validate and parse the JSON string\\n    user = User.model_validate_json(json_data)\\n    print(\\"Validated Data:\\", user)\\nexcept ValidationError as e:\\n    print(\\"Validation Error:\\", e.json())\\n\\n```\\nThis works. Pydantic has a pretty solid _json_ to data model convertor. But it has to be a valid json string. Let\'s explore further.\\n\\n```python\\n\\n# JSON representation of the data\\n# typical replies of a small LLM which does not adhere well to \'output_json\' command\\njson_data = \'\'\'\\nHere is your json\\n{\\n    \\"id\\": 123,\\n    \\"name\\": \\"Alice\\",\\n    \\"email\\": \\"alice@example.com\\"\\n}\\n\'\'\'\\n\\ntry:\\n     # Directly validate and parse the JSON string using the new method\\n    user = User.model_validate_json(json_data)\\n    print(\\"Validated Data:\\", user)\\nexcept ValidationError as e:\\n    print(\\"Validation Error:\\", e.json())\\n\\n\\n```\\n\\n\\nError is: \\n```python \\nValidation Error: [{\\"type\\":\\"json_invalid\\",\\"loc\\":[],\\"msg\\":\\"Invalid JSON: expected value at line 2 column 1\\",\\"input\\":\\"\\\\nHere is your json\\\\n{\\\\n    \\\\\\"id\\\\\\": 123,\\\\n    \\\\\\"name\\\\\\": \\\\\\"Alice\\\\\\",\\\\n    \\\\\\"email\\\\\\": \\\\\\"alice@example.com\\\\\\"\\\\n}\\\\n\\",\\"ctx\\":{\\"error\\":\\"expected value at line 2 column 1\\"},\\"url\\":\\"https://errors.pydantic.dev/2.10/v/json_invalid\\"}]\\n```\\n\\nNow, let\'s add one more step in the mix. Let\'s use the json_partial_py library to parse the JSON string. and then pass it to pydantic.\\n\\n\\n```python\\n\\nfrom json_partial_py import to_json_string # <---- this is a new import\\n\\n# typical replies of a small LLM which does not adhere well to \'output_json\' command\\njson_data = \'\'\'\\nHere is your json\\n{\\n    \\"id\\": 123,\\n    \\"name\\": \\"Alice\\",\\n    \\"email\\": \\"alice@example.com\\"\\n}\\n\'\'\'\\n\\ntry:\\n    stringified_json = to_json_string(json_data)\\n     # Directly validate and parse the JSON string using the new method\\n    user = User.model_validate_json(stringified_json)\\n    print(\\"Validated Data:\\", user)\\nexcept ValidationError as e:\\n    print(\\"Validation Error:\\", e.json())\\n\\n\\n```\\n\\nand voila!! Now you can rest assured that you will get clean json parsed from the LLM output.\\n\\nP.S. I am author of the [`json_partial_py` library](https://pypi.org/project/json_partial_python/). It was extracted from [baml project](https://github.com/BoundaryML/baml?tab=readme-ov-file)."},{"id":"string-interning-rust","metadata":{"permalink":"/byte_vault/string-interning-rust","source":"@site/blog/2025-03-01-string-interning.mdx","title":"String interning in Rust","description":"What is String Interning?","date":"2025-03-01T00:00:00.000Z","tags":[{"inline":false,"label":"Rust","permalink":"/byte_vault/tags/rust","description":"Rust lang"}],"readingTime":2.445,"hasTruncateMarker":false,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"string-interning-rust","title":"String interning in Rust","date":"2025-03-01T00:00:00.000Z","authors":["abeeshake"],"tags":["rust"],"draft":false},"unlisted":false,"prevItem":{"title":"Reliable Structured Outputs with LLMs","permalink":"/byte_vault/reliable-structured-outputs-with-llms"},"nextItem":{"title":"Rust tricks for the average developer (me)","permalink":"/byte_vault/rust-tips-tricks"}},"content":"### What is String Interning?\\n\\nString interning is a technique that ensures each unique string is stored only once in memory, reducing redundancy and improving performance.\\n\\n\\n### **Why String Interning Matters in Real-World Scenarios**  \\n\\nString interning is **critical in performance-sensitive applications** where **redundant string storage leads to memory overhead and slower lookups**. Here\u2019s why it matters in real-world scenarios:\\n\\n### **Real-World Use Cases**\\nString interning reduces memory usage and improves performance by storing each unique string only once.\\n\\nThe following table highlights some real-world use cases where string interning is beneficial:\\n\\n| Use Case                         | Example                                                                                                |\\n| -------------------------------- | ------------------------------------------------------------------------------------------------------ |\\n| Compiler and Interpreter Optimization | Rust\u2019s compiler, Python\u2019s CPython, and Java\u2019s JVM use string interning to optimize symbol tables.       |\\n| Embedded Systems & IoT             | Logging frameworks like `defmt` use interning to minimize **flash storage usage** on microcontrollers. |\\n| Web Servers & API Performance      | High-traffic APIs handling millions of requests often receive the same strings (e.g., headers, JSON keys). |\\n| Databases & Search Engines         | Search engines like Elasticsearch and databases like PostgreSQL intern frequently queried strings.        |\\n\\n\\n\\n### **How It Works**  \\n- `InternedString::new()` returns an **empty interned string**.  \\n- `InternedString::from(s)` interns a string, ensuring uniqueness.  \\n- `as_str()`, `len()`, and `is_empty()` mimic `String` methods.  \\n- A **global HashMap** stores interned strings, avoiding duplicates.  \\n- **Thread safety** is ensured using `Lazy<Mutex<HashMap>>`.  \\n\\n\\n### Implementation of String Interning in Rust\\n\\n```rust\\nuse std::collections::HashMap;\\nuse std::sync::{Arc, Mutex};\\nuse once_cell::sync::Lazy; // Ensures thread-safe global interner\\n\\n#[derive(Debug, Clone, PartialEq, Eq, Hash)]\\npub struct InternedString {\\n    inner: Arc<str>,\\n}\\n\\n// Global interner (singleton)\\nstatic INTERNER: Lazy<Mutex<HashMap<Arc<str>, ()>>> = Lazy::new(|| Mutex::new(HashMap::new()));\\n\\nimpl InternedString {\\n    /// Creates an empty interned string (similar to `String::new()`)\\n    pub fn new() -> Self {\\n        Self::intern(\\"\\")\\n    }\\n\\n    /// Interns a given string and returns an InternedString\\n    pub fn from<S: AsRef<str>>(s: S) -> Self {\\n        Self::intern(s.as_ref())\\n    }\\n\\n    /// Returns a reference to the interned string\\n    pub fn as_str(&self) -> &str {\\n        &self.inner\\n    }\\n\\n    /// Returns the length of the interned string\\n    pub fn len(&self) -> usize {\\n        self.inner.len()\\n    }\\n\\n    /// Checks if the interned string is empty\\n    pub fn is_empty(&self) -> bool {\\n        self.inner.is_empty()\\n    }\\n\\n    /// Interns a string, ensuring no duplicates exist\\n    fn intern(s: &str) -> Self {\\n        let mut interner = INTERNER.lock().unwrap();\\n\\n        // If the string is already interned, return the existing reference\\n        if let Some(existing) = interner.keys().find(|k| k.as_ref() == s) {\\n            return InternedString {\\n                inner: Arc::clone(existing),\\n            };\\n        }\\n\\n        // Otherwise, intern the new string\\n        let arc_str = Arc::from(s);\\n        interner.insert(Arc::clone(&arc_str), ());\\n        \\n        InternedString { inner: arc_str }\\n    }\\n}\\n\\n#[cfg(test)]\\nmod tests {\\n    use super::*;\\n\\n    #[test]\\n    fn test_empty_string() {\\n        let empty1 = InternedString::new();\\n        let empty2 = InternedString::new();\\n        assert_eq!(empty1, empty2);\\n        assert!(empty1.is_empty());\\n    }\\n\\n    #[test]\\n    fn test_interning() {\\n        let s1 = InternedString::from(\\"hello\\");\\n        let s2 = InternedString::from(\\"hello\\");\\n        let s3 = InternedString::from(\\"world\\");\\n\\n        assert_eq!(s1, s2);\\n        assert_ne!(s1, s3);\\n    }\\n\\n    #[test]\\n    fn test_string_length() {\\n        let s = InternedString::from(\\"test\\");\\n        assert_eq!(s.len(), 4);\\n    }\\n\\n    #[test]\\n    fn test_string_content() {\\n        let s = InternedString::from(\\"RustLang\\");\\n        assert_eq!(s.as_str(), \\"RustLang\\");\\n    }\\n}\\n```"},{"id":"rust-tips-tricks","metadata":{"permalink":"/byte_vault/rust-tips-tricks","source":"@site/blog/2025-01-18-rust-tip-tricks.mdx","title":"Rust tricks for the average developer (me)","description":"001 : &str and AsRef","date":"2025-01-18T00:00:00.000Z","tags":[{"inline":false,"label":"Rust","permalink":"/byte_vault/tags/rust","description":"Rust lang"}],"readingTime":0.62,"hasTruncateMarker":false,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"rust-tips-tricks","title":"Rust tricks for the average developer (me)","date":"2025-01-18T00:00:00.000Z","draft":false,"authors":["abeeshake"],"tags":["rust"]},"unlisted":false,"prevItem":{"title":"String interning in Rust","permalink":"/byte_vault/string-interning-rust"},"nextItem":{"title":"Streaming HTTP to Disk","permalink":"/byte_vault/streaming-http-to-disk"}},"content":"### 001 : `&str` and `AsRef<OsStr>`\\n\\nThe change from:\\n\\n```rust\\npub fn load_extension(&self, path: &str) -> Result<()>\\n```\\n\\nto:\\n\\n```rust\\npub fn load_extension<P: AsRef<std::ffi::OsStr>>(&self, path: P) -> Result<()>\\n```\\n\\nimproves flexibility and usability. The original function only accepted `&str`, requiring explicit conversion for types like `String`, `PathBuf`, or `Path`. The updated version uses a generic parameter `P` with the `AsRef<std::ffi::OsStr>` trait, allowing it to accept any type that can be referenced as an `OsStr`, such as `&str`, `String`, `Path`, or `PathBuf`.\\n\\n**Original Implementation:**\\n```rust\\nuse std::path::Path;\\n\\nlet path_str = String::from(\\"/some/path\\");\\nlet path_ref = Path::new(\\"/another/path\\");\\n\\n// Example 1: Using String\\ninstance.load_extension(path_str);\\n\\n// Example 2: Using &Path\\ninstance.load_extension(&path_ref);\\n\\n// Example 3: Using Path directly\\ninstance.load_extension(Path::new(\\"/yet/another/path\\"));\\n\\n```\\n\\nThis reduces boilerplate and improves compatibility with different path types."},{"id":"streaming-http-to-disk","metadata":{"permalink":"/byte_vault/streaming-http-to-disk","source":"@site/blog/2025-01-10-http-streaming-to-disk.mdx","title":"Streaming HTTP to Disk","description":"HTTP responses can be quite large and memory consumption can be a concern. In","date":"2025-01-10T00:00:00.000Z","tags":[{"inline":false,"label":"Rust","permalink":"/byte_vault/tags/rust","description":"Rust lang"}],"readingTime":3.42,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"streaming-http-to-disk","title":"Streaming HTTP to Disk","date":"2025-01-10T00:00:00.000Z","authors":["abeeshake"],"tags":["rust"]},"unlisted":false,"prevItem":{"title":"Rust tricks for the average developer (me)","permalink":"/byte_vault/rust-tips-tricks"},"nextItem":{"title":"Deep Flattening in Rust - Using Recursive Types ","permalink":"/byte_vault/deep-flattening-in-rust-using-recursive-types"}},"content":"HTTP responses can be quite large and memory consumption can be a concern. In\\nsome cases, it is important to be able to handle large responses without\\nloading the entire response into memory.\\n\\nOne such scenario is when you want to download a large file from a server. If\\nyou were to load the entire file into memory, it would require a large amount\\nof memory and would be inefficient. Instead, you can use a streaming approach\\nto download the file directly to disk.\\n\\nThis example will show you how to do just that using the `reqwest` and `tokio`\\ncrates (Rust). Here is the rough flow.\\n\\n\\n```mermaid\\nflowchart TD\\n    A[\\"Start: Initiate Download\\"] --\x3e B[\\"Send GET request using reqwest client\\"]\\n    B --\x3e C[\\"Receive Response object\\"]\\n    C --\x3e D[\\"Convert response body to byte stream using bytes_stream()\\"]\\n    D --\x3e E[\\"Iterate over each chunk asynchronously\\"]\\n    E --\x3e F[\\"Write each chunk to file using tokio::fs::File\\"]\\n    F --\x3e G{\\"More chunks?\\"}\\n    G -- Yes --\x3e E\\n    G -- No --\x3e H[\\"End: Data Saved to Disk\\"]\\n    \\n    style A fill:#f9f,stroke:#333,stroke-width:2px\\n    style H fill:#bbf,stroke:#333,stroke-width:2px\\n\\n```\\n\\n{/* truncate */}\\n\\n\\n## Understanding the Streaming Process\\n\\nIn the provided streaming implementation, the key to avoiding loading the entire response into memory lies in **how the response body is processed**. Let\'s break down the relevant parts of the code:\\n\\n```rust\\n// Send the GET request and get the response\\nlet resp = client\\n    .get(&url)\\n    .header(\\"User-Agent\\", \\"rust-zip-extractor\\")\\n    .send()\\n    .await?\\n    .error_for_status()?; // Ensure the request was successful\\n\\n// Convert the response body into a stream of bytes\\nlet mut stream = resp.bytes_stream();\\n\\n// Iterate over the stream and write each chunk to the file\\nwhile let Some(chunk) = stream.next().await {\\n    let data = chunk?; // Handle potential stream errors\\n    file.write_all(&data).await?;\\n}\\n```\\n\\n### Key Points:\\n\\n1. **`send().await?` Does Not Buffer the Entire Response:**\\n   - The `.send().await?` method initiates the HTTP request and returns a `Response` object **without** reading the entire response body into memory.\\n   - The response body is **lazy-loaded**, meaning it fetches data incrementally as you process the stream.\\n\\n2. **Using `bytes_stream()` for Streaming:**\\n   - The `.bytes_stream()` method converts the response body into a `Stream` of `Bytes` chunks.\\n   - **Crucially**, this stream processes the response incrementally, allowing you to handle large files without high memory consumption.\\n\\n3. **Writing Chunks Directly to Disk:**\\n   - By iterating over `stream.next().await`, you handle each chunk as it arrives and immediately write it to the file.    \\n   - **`send().await?`**: Initiates the request and prepares to receive the response without buffering the entire body.\\n   - **`bytes_stream()`**: Explicitly creates a stream that processes the response body chunk by chunk, preventing full buffering.\\n\\nTherefore, **the provided code does indeed stream the response directly to disk without loading the entire response into memory**.\\n\\nFull code:\\n\\n```rust\\nuse reqwest::Client;\\nuse std::path::PathBuf;\\nuse tokio::fs::File;\\nuse tokio::io::AsyncWriteExt;\\nuse uuid::Uuid;\\nuse futures::StreamExt;\\n\\n/// Async function to download the repository as a ZIP file by streaming the response to disk.\\n/// Returns the path to the saved ZIP file.\\n///\\n/// # Arguments\\n///\\n/// * `owner` - The owner of the GitHub repository.\\n/// * `repo` - The name of the repository.\\n/// * `reference` - The branch, tag, or commit reference.\\n///\\n/// # Errors\\n///\\n/// Returns an error if the HTTP request fails or if writing to disk fails.\\npub async fn download_repo_as_zip(\\n    owner: &str,\\n    repo: &str,\\n    reference: &str,\\n) -> Result<PathBuf, Box<dyn std::error::Error>> {\\n    let url = format!(\\n        \\"https://api.github.com/repos/{owner}/{repo}/zipball/{reference}\\",\\n        owner = owner,\\n        repo = repo,\\n        reference = reference\\n    );\\n    let client = Client::new();\\n\\n    // GitHub requires a User-Agent header\\n    let resp = client\\n        .get(&url)\\n        .header(\\"User-Agent\\", \\"rust-zip-extractor\\")\\n        .send()\\n        .await?\\n        .error_for_status()?; // Ensure the request was successful\\n\\n    // Generate a unique filename using UUID\\n    let filename = format!(\\"{}_{}_{}.zip\\", owner, repo, Uuid::new_v4());\\n    let filepath = PathBuf::from(&filename);\\n\\n    // Create the file asynchronously\\n    let mut file = File::create(&filepath).await?;\\n\\n    // Convert the response body into a stream of bytes\\n    let mut stream = resp.bytes_stream();\\n\\n    // Iterate over the stream and write each chunk to the file\\n    while let Some(chunk) = stream.next().await {\\n        let data = chunk?; // Handle potential stream errors\\n        file.write_all(&data).await?;\\n    }\\n\\n    // Ensure all data is written to disk\\n    file.flush().await?;\\n\\n    Ok(filepath)\\n}\\n```"},{"id":"deep-flattening-in-rust-using-recursive-types","metadata":{"permalink":"/byte_vault/deep-flattening-in-rust-using-recursive-types","source":"@site/blog/2024-12-31-rust-deep-flatten.mdx","title":"Deep Flattening in Rust - Using Recursive Types ","description":"Deep Flattening in Rust: A Recursive Adventure","date":"2024-12-31T00:00:00.000Z","tags":[{"inline":false,"label":"Rust","permalink":"/byte_vault/tags/rust","description":"Rust lang"}],"readingTime":2.995,"hasTruncateMarker":true,"authors":[{"name":"Joel Medicala","url":"https://github.com/JoeruCodes","page":{"permalink":"/byte_vault/authors/joel-medicala"},"socials":{"x":"https://x.com/JoeruCodes","github":"https://github.com/JoeruCodes"},"imageURL":"https://github.com/JoeruCodes.png","key":"joel-medicala"},{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"deep-flattening-in-rust-using-recursive-types","title":"Deep Flattening in Rust - Using Recursive Types ","date":"2024-12-31T00:00:00.000Z","authors":["joel-medicala","abeeshake"],"tags":["rust"]},"unlisted":false,"prevItem":{"title":"Streaming HTTP to Disk","permalink":"/byte_vault/streaming-http-to-disk"},"nextItem":{"title":"Caddy Reverse Proxy Performance: 300% Boost with Unix Sockets","permalink":"/byte_vault/how-to-solve-reverse-proxy-performance-issues-in-caddy-server-a-300-performance-boost-using-unix-sockets"}},"content":"### Deep Flattening in Rust: A Recursive Adventure\\n\\nFlattening nested data structures is a common problem in programming. However, flattening structures with an arbitrary depth\u2014like nested `Vec`s within `Vec`s\u2014can be tricky. Rust, with its strong type system and trait-based polymorphism, allows us to implement elegant solutions to such problems. In this post, we\'ll explore a recursive approach to deep flattening in Rust using traits, type inference, and iterators.\\n\\n#### The Goal\\n\\nGiven a deeply nested structure, such as:\\n\\n```rust\\nlet nested_vec = vec![\\n    vec![vec![1, 2, 3], vec![4, 5]],\\n    vec![vec![6], vec![7, 8, 9]],\\n];\\n```\\n\\nOur goal is to flatten it into:\\n\\n```rust\\nlet flattened = vec![1, 2, 3, 4, 5, 6, 7, 8, 9];\\n```\\n{/* truncate */}\\n\\nThe depth of nesting is not fixed\u2014it could be `Vec<Vec<Vec<T>>>`, `Vec<Vec<Vec<Vec<T>>>>`, or deeper.\\n\\n---\\n\\n### TL;DR: high level idea\\n\\nRust\u2019s iterators and traits allow us to create a type-safe, recursive implementation to handle deep flattening. The solution uses three key components:\\n\\n1. **The **Trait**: A recursive trait defining how to flatten iterators.\\n2. **Base and Recursive Implementations**: Separate implementations for handling the base case (non-nested items) and recursive case (nested items).\\n3. **A Wrapper Struct**: A helper type to simplify type inference.\\n\\n---\\n\\n### Implementation\\n\\nThe fun part lies in using Rust\'s types as in recursive way.\\n\\n#### The `DeepFlattenIteratorOf` Trait\\n\\nThis trait defines the recursive structure of our flattening logic:\\n\\n```rust\\npub trait DeepFlattenIteratorOf<Depth, T> {\\n    type DeepFlatten: Iterator<Item = T>;\\n    fn deep_flatten(this: Self) -> Self::DeepFlatten;\\n}\\n```\\n\\n- `Depth` tracks the nesting level.\\n- `T` is the type of the innermost element.\\n- `DeepFlatten` is the resulting iterator after flattening.\\n\\n#### Base Case: No Nesting\\n\\nThe base condition stops recursion when the structure is already flat:\\n\\n```rust\\nimpl<I: Iterator> DeepFlattenIteratorOf<(), I::Item> for I {\\n    type DeepFlatten = Self;\\n\\n    fn deep_flatten(this: Self) -> Self::DeepFlatten {\\n        this\\n    }\\n}\\n```\\n\\nHere, when `Depth` is `()`, no further flattening is needed, and the iterator is returned as-is.\\n\\n#### Recursive Case: Flatten Nested Items\\n\\nFor nested structures, the recursion continues until reaching the base case:\\n\\n```rust\\nimpl<Depth, I, T> DeepFlattenIteratorOf<(Depth,), T> for I\\nwhere\\n    Flatten<I>: DeepFlattenIteratorOf<Depth, T>,\\n    I: Iterator,\\n    I::Item: IntoIterator,\\n{\\n    type DeepFlatten = <Flatten<I> as DeepFlattenIteratorOf<Depth, T>>::DeepFlatten;\\n\\n    fn deep_flatten(this: Self) -> Self::DeepFlatten {\\n        DeepFlattenIteratorOf::deep_flatten(this.flatten())\\n    }\\n}\\n```\\n\\n- `Flatten<I>` handles one level of flattening.\\n- The recursion continues until it reaches the base case.\\n\\n#### Wrapper Struct for Type Inference\\n\\nThe `DeepFlatten` struct simplifies type inference by wrapping the recursive logic:\\n\\n```rust\\npub struct DeepFlatten<Depth, I, T>\\nwhere\\n    I: DeepFlattenIteratorOf<Depth, T>,\\n{\\n    inner: I::DeepFlatten,\\n}\\n\\nimpl<I: Iterator> DeepFlattenExt for I {}\\n```\\n\\nThis allows users to call the `.deep_flatten()` method directly:\\n\\n```rust\\npub trait DeepFlattenExt: Iterator + Sized {\\n    fn deep_flatten<Depth, T>(self) -> DeepFlatten<Depth, Self, T>\\n    where\\n        Self: DeepFlattenIteratorOf<Depth, T>,\\n    {\\n        DeepFlatten {\\n            inner: DeepFlattenIteratorOf::deep_flatten(self),\\n        }\\n    }\\n}\\n```\\n\\n#### Iterator Implementation for `DeepFlatten`\\n\\nFinally, the iterator implementation allows seamless iteration over flattened items:\\n\\n```rust\\nimpl<Depth, I, T> Iterator for DeepFlatten<Depth, I, T>\\nwhere\\n    I: DeepFlattenIteratorOf<Depth, T>,\\n{\\n    type Item = T;\\n\\n    fn next(&mut self) -> Option<Self::Item> {\\n        self.inner.next()\\n    }\\n}\\n```\\n\\n---\\n\\n### Example Usage\\n\\nHere\u2019s how you can use the `deep_flatten` method to flatten nested structures:\\n\\n```rust\\nfn main() {\\n    let nested_vec = vec![\\n        vec![vec![1, 2, 3], vec![4, 5]],\\n        vec![vec![6], vec![7, 8, 9]],\\n    ];\\n\\n    let flattened: Vec<i32> = nested_vec.into_iter().deep_flatten().collect();\\n\\n    assert_eq!(flattened, vec![1, 2, 3, 4, 5, 6, 7, 8, 9]);\\n\\n    println!(\\"Flattened result: {:?}\\", flattened);\\n}\\n```\\n\\n---\\n\\nThis code gist was prepared by Joel is [available on rust playground](https://play.rust-lang.org/?version=stable&mode=debug&edition=2021&gist=dbd26d3c4e89abbf50cde86dec296cd7)!\\n\\nThanks Joel once again for bringing light to this pattern! \\nThat\'s a wrap for this year! \\n\\nSee you in next year!"},{"id":"how-to-solve-reverse-proxy-performance-issues-in-caddy-server-a-300-performance-boost-using-unix-sockets","metadata":{"permalink":"/byte_vault/how-to-solve-reverse-proxy-performance-issues-in-caddy-server-a-300-performance-boost-using-unix-sockets","source":"@site/blog/2024-12-23-caddy-performance.mdx","title":"Caddy Reverse Proxy Performance: 300% Boost with Unix Sockets","description":"A recent GitHub issue  #6751  in the Caddy server repository revealed an interesting performance bottleneck when using multiple layers of reverse proxying. Here\'s what was discovered and how it was resolved.","date":"2024-12-23T00:00:00.000Z","tags":[{"inline":false,"label":"Performance","permalink":"/byte_vault/tags/performance","description":"Blog posts related to improving and understanding performance."},{"inline":false,"label":"Caddy","permalink":"/byte_vault/tags/caddy","description":"caddy config and tuning"}],"readingTime":3.965,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"how-to-solve-reverse-proxy-performance-issues-in-caddy-server-a-300-performance-boost-using-unix-sockets","title":"Caddy Reverse Proxy Performance: 300% Boost with Unix Sockets","date":"2024-12-23T00:00:00.000Z","authors":["abeeshake"],"tags":["performance","caddy"]},"unlisted":false,"prevItem":{"title":"Deep Flattening in Rust - Using Recursive Types ","permalink":"/byte_vault/deep-flattening-in-rust-using-recursive-types"},"nextItem":{"title":"1brc - same tricks across languages","permalink":"/byte_vault/1brc-same-tricks-across-languages"}},"content":"A recent GitHub issue  [#6751](https://github.com/caddyserver/caddy/issues/6751)  in the Caddy server repository revealed an interesting performance bottleneck when using multiple layers of reverse proxying. Here\'s what was discovered and how it was resolved.\\n\\n{/* truncate */}\\n\\n## The Problem\\n\\nA user reported significant performance degradation when implementing multiple layers of reverse proxies in Caddy v2.8.4. The setup consisted of a chain of reverse proxies:\\n\\n- Port 8081: Serving static files\\n- Port 8082: Proxying to 8081\\n- Port 8083: Proxying to 8082\\n- Port 8084: Proxying to 8083\\n\\nWhen testing with a 1000MB file download, the performance metrics showed a clear pattern of degradation:\\n\\n### Multi-Threading Performance Impact\\n\\n- Direct file server (8081): ~300 Mbps with 5 threads\\n- First proxy layer (8082): ~60 Mbps with 5 threads\\n- Second proxy layer (8083): ~16 Mbps with 5 threads\\n- Third proxy layer (8084): ~16 Mbps with 5 threads\\n\\nWhat made this particularly interesting was that the server\'s CPU usage remained surprisingly low (1-5%), suggesting that the bottleneck wasn\'t in processing power.\\n\\n## The Investigation\\n\\nThe investigation, led by Caddy maintainers including Matt Holt, involved:\\n\\n1. Gathering system metrics\\n2. Analyzing CPU and memory profiles\\n3. Testing different network configurations\\n4. Examining kernel settings\\n\\n### Table 1: System Metrics\\n\\n\\n| **Commands**                                 | **Why It Is Relevant to Debugging**                                                                                               | **Output and Conclusion**                                                                                                                                                                                                                                                |\\n| -------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| `ulimit -a`                                  | Checks system limits such as maximum number of open files and other resource constraints that could impact performance.           | No bottlenecks identified in file descriptors or resource limits.                                                                                                                                                                                                        |\\n| `sysctl -p`                                  | Confirms network-related kernel parameters such as buffer sizes, default queuing discipline, and TCP congestion control settings. | <br/>`net.core.rmem_max = 2097152`<br/>`net.core.wmem_max = 2097152`<br/>`net.core.default_qdisc = fq`<br/>`net.ipv4.tcp_congestion_control = bbr`<br/><br/> Settings were optimized for high-speed networking.<br />TCP congestion control was correctly set to`bbr`. |\\n| General hardware specs (CPU, RAM, NIC, etc.) | baseline hardware information                                                                                                     | Verified adequate resources (1 Core Ryzen 5950X, 1024MB RAM, 10Gbps NIC). No resource-related constraints.                                                                                                                                                               |\\n\\n---\\n\\n### Table 2: Profile Analysis\\n\\n\\n| **Commands**                                                  | **Why It Is Relevant to Debugging**                                                                | **Output and Conclusion**                                                                          |\\n| --------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------- |\\n| Attempted to collect goroutine profiles                       | Helps identify bottlenecks or inefficiencies in goroutines that may be causing performance issues. | Could not identify significant bottlenecks in goroutines.                                          |\\n| Accessed CPU Profile via browser                              | Provides CPU usage details to determine if high CPU usage is a factor affecting performance.       | No high CPU usage detected. CPU load was between 1-5%.                                             |\\n| `wget http://127.0.0.1:2019/debug/pprof/profile?seconds=1000` | Downloads detailed CPU profiles for offline analysis.                                              | Profiles downloaded successfully. Further analysis confirmed no CPU bottlenecks or inefficiencies. |\\n| Collected heap profiles                                       | Helps analyze memory usage and potential leaks in the application.                                 | Memory usage was within acceptable limits, with no indication of memory leaks.                     |\\n\\n---\\n\\n### Table 3: Network Testing\\n\\n\\n| **Commands**                                                                           | **Why It Is Relevant to Debugging**                                                          | **Output and Conclusion**                                               |\\n| -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- |\\n| Tests from multiple locations (Singapore, Los Angeles, Seoul)                          | Evaluates network performance across different regions to identify geographical bottlenecks. | Performance was consistent across all regions.                          |\\n| Tests with different file sizes (100MiB, 1000MiB)                                      | Determines if performance issues are related to file size or payload.                        | No significant performance variance with different file sizes.          |\\n| `curl -o /dev/null http://host.domain:port/1000MiB`                                    | Single-threaded test evaluates download performance under minimal concurrency.               | acceptable network speed                                                |\\n| `echo 1 1 1 1 1 \\\\| xargs -n1 -P5 curl -s -o /dev/null http://host.domain:port/1000MiB` |                                                                                              | Multi-threaded test assesses network performance under concurrent load. |\\n\\n\\n---\\n\\n### Table 4: Kernel Analysis\\n\\n\\n| **Commands**                                       | **Why It Is Relevant to Debugging**                                                                  | **Output and Conclusion**                                         |\\n| -------------------------------------------------- | ---------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |\\n| Checked systemd service file settings              | Confirms that the maximum number of open files is sufficient for high-concurrency workloads.         | Verified`LimitNOFILE=1048576`. No issues found.                   |\\n\\n\\n## The Solution\\n\\nThe breakthrough came when testing with Unix sockets instead of TCP connections. By modifying the Caddyfile to use Unix sockets for inter-process communication, the performance issues were completely resolved. Here\'s what the optimized configuration looked like:\\n\\n```\\n:8081 {\\n    bind 0.0.0.0 unix//dev/shm/8081.sock\\n    file_server browse\\n    root * /opt/www\\n}\\n\\n:8082 {\\n    bind 0.0.0.0 unix//dev/shm/8082.sock\\n    reverse_proxy unix//dev/shm/8081.sock\\n}\\n\\n:8083 {\\n    bind 0.0.0.0 unix//dev/shm/8083.sock\\n    reverse_proxy unix//dev/shm/8082.sock\\n}\\n\\n:8084 {\\n    reverse_proxy unix//dev/shm/8083.sock\\n}\\n```\\n\\n## Key Takeaways\\n\\n1. TCP connection overhead can significantly impact performance in multi-layer reverse proxy setups\\n2. Unix sockets provide a more efficient alternative for local inter-process communication\\n3. Low CPU usage doesn\'t always mean optimal performance - network stack overhead can be the bottleneck\\n4. When dealing with multiple local reverse proxies, consider using Unix sockets instead of TCP connections"},{"id":"1brc-same-tricks-across-languages","metadata":{"permalink":"/byte_vault/1brc-same-tricks-across-languages","source":"@site/blog/2024-12-22-1brc-common-learnings.mdx","title":"1brc - same tricks across languages","description":"The 1 Billion Row Challenge (1BRC) is a programming challenge focused on processing a large dataset of temperature measurements. If you\'re unfamiliar with it, you can learn more from these resources: 1 and 2.","date":"2024-12-22T00:00:00.000Z","tags":[{"inline":false,"label":"Performance","permalink":"/byte_vault/tags/performance","description":"Blog posts related to improving and understanding performance."}],"readingTime":2.16,"hasTruncateMarker":true,"authors":[{"name":"Abhishek Tripathi","title":"Curiosity brings awareness.","url":"https://github.com/TwistingTwists","page":{"permalink":"/byte_vault/authors/abeeshake"},"socials":{"x":"https://x.com/twistin456","github":"https://github.com/TwistingTwists"},"imageURL":"https://github.com/TwistingTwists.png","key":"abeeshake"}],"frontMatter":{"slug":"1brc-same-tricks-across-languages","title":"1brc - same tricks across languages","date":"2024-12-22T00:00:00.000Z","authors":["abeeshake"],"tags":["performance"]},"unlisted":false,"prevItem":{"title":"Caddy Reverse Proxy Performance: 300% Boost with Unix Sockets","permalink":"/byte_vault/how-to-solve-reverse-proxy-performance-issues-in-caddy-server-a-300-performance-boost-using-unix-sockets"}},"content":"The 1 Billion Row Challenge (1BRC) is a programming challenge focused on processing a large dataset of temperature measurements. If you\'re unfamiliar with it, you can learn more from these resources: [1](https://github.com/gunnarmorling/1brc) and [2](https://www.morling.dev/blog/1brc-results-are-in/).\\n\\nThis is a cheatsheet  of optimisations done for 1brc challenges. It tries to summarise and put the optimisations in perspective.\\n\\n{/* truncate */}\\n\\n### Data encoding / parsing\\n\\n\\n| Trick                                         | Outcome                                                                                     | Note                                                                                                                                                 |\\n| :---------------------------------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |\\n| converting contents of file as string: don\'t! | [~10% perf](https://github.com/gunnarmorling/1brc/discussions/57#discussioncomment-8153186) | Don\'t convert the contents of the file to`String`. Simply process raw bytes.                                                                         |\\n| parsing float values<br />                    |                                                                                             | parsing after assuming only one place after decimal.                                                                                                 |\\n| parsing integer                               |                                                                                             | [Branchless Programming via Bit Manipulation](https://youtu.be/EFXxXFHpS0M?t=1255)<br /> this is not generalizable                                   |\\n| parsing the city name (finding`;` separator)  |                                                                                             | If the data looks like`Tokio;13.4` and we want to find `;`<br/><br/>using 8 operations, you can find `;`.<br /><br /><br />SWAR = SIMD as a register |\\n\\n### Reading files from disk\\n\\n\\n| Trick                                                                                                                                  | Outcome                                                                                     | Note                                                                                                                                                                                                                    |\\n| ---------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| Reading in chunks[golang](https://www.bytesizego.com/blog/one-billion-row-challenge-go#:~:text=Reading%20file%3A%20Read%20in%20chunks) | [~10% perf](https://github.com/gunnarmorling/1brc/discussions/57#discussioncomment-8153186) |                                                                                                                                                                                                                         |\\n| mmap                                                                                                                                   |                                                                                             | [mmap the input file. Split input based on core count and run it with a thread per core.](https://github.com/gunnarmorling/1brc/discussions/57#discussioncomment-8041416)<br/><br/>mmap is an unsafe operation in Rust. |\\n\\n### Float handling\\n\\n\\n| Trick                                        | Outcome                                                                                                         | Notes |\\n| ---------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | ------- |\\n| Don\'t do floating point operations. Use int. | [20% speed gains](https://github.com/gunnarmorling/1brc/discussions/57#discussioncomment-8024568)               |       |\\n| Use fixed size integers instead of float     | [~10% gain in golang](https://benhoyt.com/writings/go-1brc/#:~:text=Solution%204%3A%20fixed%20point%20integers) |       |\\n\\n### The Hashmap - simpler hash function\\n\\n\\n| Trick                                                                                                                                                                                                                        | Outcome                                                                                                                                                                                                                        | Note                                                                                                                                                                                                                                                                                                                                                                                        |\\n| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| [Fixed Size hash table 10k](https://github.com/gunnarmorling/1brc/discussions/57#:~:text=Treat%20the%20first%20eight%20bytes%20of%20the%20name%20as%20a%20hash%20key%20into%20a%20fixed%20size%2010k%20item%20hash%20table.) | [~40% gain - the custom hash table cuts down the time from 41.3 seconds to 22.1s.](https://benhoyt.com/writings/go-1brc/#:~:text=the%20custom%20hash%20table%20cuts%20down%20the%20time%20from%2041.3%20seconds%20to%2022.1s.) | How to resolve collisions?<br /><br />- Find first unoccupied slot<br />- <br />[if hash collide, goto next empty slot algorithm](https://benhoyt.com/writings/go-****1brc/#:~:text=It%E2%80%99s%20a%20simple%20implementation%20that%20uses%20the%20FNV%2D1a%20hash%20algorithm%20with%20linear%20probing%3A%20if%20there%E2%80%99s%20a%20collision%2C%20use%20the%20next%20empty%20slot.) |\\n| Hash key - Integer, not String                                                                                                                                                                                               | [4x gain in a different competition](https://youtu.be/SVw9nKfVPx4?t=501)                                                                                                                                                       | the time is spent in`Hashmap.get`.<br /> Not in 1brc, but in [other context](https://www.youtube.com/watch?v=SVw9nKfVPx4&t=501s), Hashmap.get was the bottleneck. hence this optimisation makes a lot of difference there, not in 1brc.                                                                                                                                                     |\\n\\n### With OS threads and parallelism - go brr!\\n\\n\\n| Trick                                                        | Outcome                                                                                                                                                                                                                                  | Notes                                                     |\\n| -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------- |\\n| address the go routine + rust tokio task + erlang processes? | [~ 4-6x gains](https://benhoyt.com/writings/go-1brc/#:~:text=Processing%20the%20input%20file%20in%20parallel%20provides%20a%20huge%20win%20over%20r1%2C%20taking%20the%20time%20from%201%20minute%2045%20seconds%20to%2022.6%20seconds.) | biggest gains are obtained by utilizing OS threads fully! |"}]}}')}}]);